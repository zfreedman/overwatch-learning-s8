{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwatch Battletag Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib2\n",
    "from pathlib2 import Path\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discovery_file_str = \"../data/discovery.csv\"\n",
    "discovery_curr_page_file_str = \"../data/discovery_curr_page.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_curr_page_file(file_str):\n",
    "    \"\"\"Writes the current page file to the discovery current page file.\"\"\"\n",
    "    f = open(file_str, \"w\")\n",
    "    f.write(str(curr_page))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current page file FOUND. Resuming scraping from page 1647\n"
     ]
    }
   ],
   "source": [
    "#Discover competitive Overwatch players and scrape their info\n",
    "#for more detailed scraping at a later date.\n",
    "tracker_str = \"https://overwatchtracker.com/leaderboards/pc/global/CompetitiveRank?page={}&mode=1\"\n",
    "\n",
    "#Get page where parsing left off last\n",
    "curr_page = -1\n",
    "try:\n",
    "    path = Path(discovery_curr_page_file_str).resolve()\n",
    "except:\n",
    "    print(\"Current page file NOT FOUND. Using current page value of 1.\")\n",
    "    curr_page = 1\n",
    "else:\n",
    "    discovery_curr_page_file = open(discovery_curr_page_file_str, \"r\")\n",
    "    curr_page = int(discovery_curr_page_file.readline())\n",
    "    print(\"Current page file FOUND. Resuming scraping from page {}\".format(curr_page))\n",
    "    discovery_curr_page_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Store battle tag tuple info here\n",
    "battletags = []\n",
    "#Writing condition\n",
    "write_every_n_pages = 3\n",
    "#Stopping condition\n",
    "no_sr_found = False\n",
    "\n",
    "#Parse Overwatch Tracker\n",
    "print(\"Starting scraping ...\")\n",
    "print(\"Writing players to file every {} pages ...\".format(write_every_n_pages))\n",
    "while curr_page < 3000:\n",
    "    #Log\n",
    "    print(\"Scraping page {} ...\".format(curr_page))\n",
    "    \n",
    "    #Below request line assigns a header to avoid getting banned xd\n",
    "    req = urllib2.Request(tracker_str.format(curr_page), headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    soup = bs(urllib2.urlopen(req).read())\n",
    "    battletag_htmls = soup.find_all(\"a\", {\"data-tooltip\": \"notooltip\"})\n",
    "    for html in battletag_htmls:\n",
    "        #End of placed players list (people who've started placement matches but haven't finished)\n",
    "        if(html.find_next_sibling().getText() == \"\"):\n",
    "            no_sr_found = True\n",
    "            break;\n",
    "            \n",
    "        #Add battletag entry (last entry means 0=not scraped, 1= scraped)\n",
    "        battletags.append((html.parent.previous_sibling.find_previous_sibling().getText(),\n",
    "                           html.getText(),\n",
    "                           html.find_next_sibling().getText(),\n",
    "                           0))       \n",
    "            \n",
    "    #Stop scraping condition\n",
    "    if no_sr_found:\n",
    "        print(\"Scraping completed on page {}\".format(curr_page))\n",
    "        #Write last page scraped to file\n",
    "        update_curr_page_file(discovery_curr_page_file_str)\n",
    "        break;\n",
    "    \n",
    "    #Update current page\n",
    "    curr_page += 1\n",
    "    \n",
    "    #Save data to file every n pages\n",
    "    if curr_page % write_every_n_pages == 0:\n",
    "        print(\"Writing battletags ...\")\n",
    "        #Update discovery file\n",
    "        with codecs.open(discovery_file_str, \"a\", encoding=\"utf8\") as f:\n",
    "            for b in battletags:\n",
    "                f.write(str(b[0]) + \",\" + str(b[1]) + \",\" + str(b[2]) + \",\" + str(b[3]) + \"\\n\")\n",
    "        #Clear out discovery\n",
    "        battletags = []\n",
    "        print(\"Battletags written!!\")\n",
    "        \n",
    "        #Update current page file\n",
    "        print(\"Writing current page ...\")\n",
    "        update_curr_page_file(discovery_curr_page_file_str)\n",
    "        print(\"Current page written!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
